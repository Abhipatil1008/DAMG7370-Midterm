{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc13bec5-520f-47b7-8b50-33c5fa8c65ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def gold_inspection_df():\n",
    "    return spark.read.table(\"workspace.`damg-midterm`.gold_inspection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b36bb95-fae4-462f-ab0c-9cb8a7e36649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_date\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_date():\n",
    "    # Fixed start date and dynamic end date (today)\n",
    "    start_date = F.to_date(F.lit(\"2021-01-01\"))\n",
    "    end_date = F.current_date()\n",
    "\n",
    "    # Generate one row per day from 2021-01-01 to today\n",
    "    df = (\n",
    "        spark.createDataFrame([(1,)], [\"dummy\"])\n",
    "        .select(F.sequence(start_date, end_date).alias(\"date_seq\"))\n",
    "        .select(F.explode(\"date_seq\").alias(\"Full_Date\"))\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"Date_key\", F.date_format(\"Full_Date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "          .withColumn(\"Year\", F.year(\"Full_Date\"))\n",
    "          .withColumn(\"Quarter\", F.quarter(\"Full_Date\"))\n",
    "          .withColumn(\"Month\", F.month(\"Full_Date\"))\n",
    "          .withColumn(\"Day\", F.dayofmonth(\"Full_Date\"))\n",
    "          .withColumn(\"Day_of_Week\", F.dayofweek(\"Full_Date\"))\n",
    "          .withColumn(\"Is_Weekend\", F.col(\"Day_of_Week\").isin(1, 7).cast(\"int\"))\n",
    "          .withColumn(\"Fiscal_Year\", F.year(\"Full_Date\"))  # change if your FY is different\n",
    "          .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Date_key\",\n",
    "        \"Full_Date\",\n",
    "        \"Year\",\n",
    "        \"Quarter\",\n",
    "        \"Month\",\n",
    "        \"Day\",\n",
    "        \"Day_of_Week\",\n",
    "        \"Is_Weekend\",\n",
    "        \"Fiscal_Year\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850d6071-aa1f-4abe-9246-6e6c93dd4d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dim_location\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_location():\n",
    "    src = gold_inspection_df()\n",
    "\n",
    "    base = (\n",
    "        src.select(\n",
    "            F.col(\"location_address\").alias(\"Street_Number\"),\n",
    "            F.col(\"src_city\").alias(\"City\"),\n",
    "            F.col(\"state\").alias(\"State\"),\n",
    "            F.col(\"zip_code\").alias(\"Zip_Code\"),\n",
    "            F.col(\"latitude\").alias(\"Latitude\"),\n",
    "            F.col(\"longitude\").alias(\"Longitude\")\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Define a deterministic ordering for assigning keys\n",
    "    w = Window.orderBy(\n",
    "        F.col(\"City\"),\n",
    "        F.col(\"State\"),\n",
    "        F.col(\"Zip_Code\"),\n",
    "        F.col(\"Street_Number\")\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        base\n",
    "        .withColumn(\"Location_Key\", F.row_number().over(w))  # starts at 1\n",
    "        .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Location_Key\",\n",
    "        \"Street_Number\",\n",
    "        \"City\",\n",
    "        \"State\",\n",
    "        \"Zip_Code\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e497429-8268-4a64-9f9f-4dd1593d5221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_inspection_type\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_inspection_type():\n",
    "    src = gold_inspection_df()\n",
    "\n",
    "    # Get distinct non-null inspection types\n",
    "    base = (\n",
    "        src.select(F.col(\"inspection_type\").alias(\"Inspection_Type\"))\n",
    "           .where(F.col(\"inspection_type\").isNotNull())\n",
    "           .distinct()\n",
    "    )\n",
    "\n",
    "    # Assign keys starting from 1 in a stable order\n",
    "    w = Window.orderBy(F.col(\"Inspection_Type\"))\n",
    "\n",
    "    df = (\n",
    "        base\n",
    "        .withColumn(\"Inspection_Type_Key\", F.row_number().over(w))  # 1,2,3,...\n",
    "        .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Inspection_Type_Key\",\n",
    "        \"Inspection_Type\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b348e7e9-5c2d-496a-adcd-6c02608d3cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_inspection_result\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_inspection_result():\n",
    "    src = gold_inspection_df()\n",
    "\n",
    "    # Distinct non-null inspection results\n",
    "    base = (\n",
    "        src.select(F.col(\"result_desc\").alias(\"Result_Desc\"))\n",
    "           .where(F.col(\"result_desc\").isNotNull())\n",
    "           .distinct()\n",
    "    )\n",
    "\n",
    "    # Assign surrogate keys starting from 1\n",
    "    w = Window.orderBy(F.col(\"Result_Desc\"))\n",
    "\n",
    "    df = (\n",
    "        base\n",
    "        .withColumn(\"Result_Key\", F.row_number().over(w))  # 1,2,3,...\n",
    "        .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Result_Key\",\n",
    "        \"Result_Desc\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f50da0-6e08-4a3d-a231-7b1cd7f9ca22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_risk_category\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_risk_category():\n",
    "    src = gold_inspection_df()\n",
    "\n",
    "    base = (\n",
    "        src.select(\n",
    "            F.col(\"risk_category\").alias(\"Risk_Desc\"),\n",
    "            F.col(\"risk_level_num\").alias(\"Risk_Level_Num\")\n",
    "        )\n",
    "        .where(F.col(\"Risk_Desc\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Assign keys starting from 1 in a stable order\n",
    "    w = Window.orderBy(F.col(\"Risk_Desc\"), F.col(\"Risk_Level_Num\"))\n",
    "\n",
    "    df = (\n",
    "        base\n",
    "        .withColumn(\"Risk_Category_Key\", F.row_number().over(w))  # 1,2,3,...\n",
    "        .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Risk_Category_Key\",\n",
    "        \"Risk_Desc\",\n",
    "        \"Risk_Level_Num\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae9fc5b-453e-4ab9-9454-364f06a36924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_violation\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_violation():\n",
    "    src = gold_inspection_df()\n",
    "\n",
    "    base = (\n",
    "        src.select(\n",
    "            F.col(\"violation_code\").cast(\"int\").alias(\"Violation_Code\"),\n",
    "            F.col(\"violation_desc\").alias(\"Violation_Desc\")\n",
    "        )\n",
    "        # remove the null / bad violation rows\n",
    "        .where(F.col(\"Violation_Code\").isNotNull())\n",
    "        .where(F.col(\"Violation_Desc\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # keys starting from 1\n",
    "    w = Window.orderBy(F.col(\"Violation_Code\"))\n",
    "\n",
    "    df = (\n",
    "        base\n",
    "        .withColumn(\"Violation_Key\", F.row_number().over(w))  # 1,2,3,...\n",
    "        .withColumn(\"Source_System\", F.lit(\"FOOD_INSPECTIONS\"))\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Violation_Key\",\n",
    "        \"Violation_Code\",\n",
    "        \"Violation_Desc\",\n",
    "        \"Source_System\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Create_Dim_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
