{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ecb362-d0a4-4af4-a952-3759a62fba92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "991931b2-bf5f-4062-a305-d0c263ea44d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def silver_df():\n",
    "    return spark.read.table(\"workspace.`damg7370-la-crime`.crime_silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b6ada42-21e9-4a8e-b011-154f1c385777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dim_date\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_date():\n",
    "\n",
    "    df = dlt.read(\"crime_silver\")\n",
    "\n",
    "    # get min/max date\n",
    "    bounds = df.select(\n",
    "        F.min(\"DATE_OCC\").alias(\"min_date\"),\n",
    "        F.max(\"DATE_OCC\").alias(\"max_date\")\n",
    "    ).first()\n",
    "\n",
    "    start_date = bounds[\"min_date\"]\n",
    "    end_date = bounds[\"max_date\"]\n",
    "\n",
    "    # generate full calendar\n",
    "    calendar = (\n",
    "        spark.range(1)\n",
    "            .select(F.sequence(F.lit(start_date), F.lit(end_date), F.expr(\"INTERVAL 1 DAY\")).alias(\"date_list\"))\n",
    "            .withColumn(\"Full_Date\", F.explode(\"date_list\"))\n",
    "    )\n",
    "\n",
    "    final = (\n",
    "        calendar\n",
    "        .withColumn(\"Date_Key\", F.date_format(\"Full_Date\", \"yyyyMMdd\").cast(\"int\"))\n",
    "        .withColumn(\"Day_of_week\", F.dayofweek(\"Full_Date\"))\n",
    "        .withColumn(\"Day_Name\", F.date_format(\"Full_Date\", \"EEEE\"))\n",
    "        .withColumn(\"Month_number\", F.month(\"Full_Date\"))\n",
    "        .withColumn(\"Month_Name\", F.date_format(\"Full_Date\", \"MMMM\"))\n",
    "        .withColumn(\"Quarter\", F.quarter(\"Full_Date\"))\n",
    "        .withColumn(\"Year\", F.year(\"Full_Date\"))\n",
    "        .withColumn(\"is_weekend\", F.when(F.dayofweek(\"Full_Date\").isin(1,7), True).otherwise(False))\n",
    "        .withColumn(\"is_holiday\", F.lit(False))\n",
    "        \n",
    "    )\n",
    "\n",
    "    return final.select(\n",
    "        \"Date_Key\", \"Full_Date\", \"Day_of_week\", \"Day_Name\",\n",
    "        \"Month_number\", \"Month_Name\", \"Quarter\", \"Year\",\n",
    "        \"is_weekend\", \"is_holiday\"\n",
    "    ).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee47f16-cc40-4513-89b7-fc70a7e13e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"dim_time\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_time():\n",
    "\n",
    "    df = dlt.read(\"crime_silver\")\n",
    "\n",
    "    # TIME_OCC comes as integer HHMM, we convert using math instead of strings (PHOTON-SAFE)\n",
    "    df = df.withColumn(\"time_int\", F.col(\"TIME_OCC\").cast(\"int\"))\n",
    "\n",
    "    # hour = FLOOR(HHMM / 100)\n",
    "    df = df.withColumn(\"hour_24\", (F.col(\"time_int\") / 100).cast(\"int\"))\n",
    "\n",
    "    # minute = HHMM % 100\n",
    "    df = df.withColumn(\"minute\", (F.col(\"time_int\") % 100).cast(\"int\"))\n",
    "\n",
    "    # Time_Key = hour * 100 + minute\n",
    "    df = df.withColumn(\"Time_Key\", (F.col(\"hour_24\") * 100 + F.col(\"minute\")).cast(\"int\"))\n",
    "\n",
    "    # Full_time = hour:minute â†’ build WITHOUT format_string()\n",
    "    df = df.withColumn(\n",
    "        \"Full_time\",\n",
    "        F.concat_ws(\n",
    "            \":\",\n",
    "            F.lpad(F.col(\"hour_24\").cast(\"string\"), 2, \"0\"),\n",
    "            F.lpad(F.col(\"minute\").cast(\"string\"), 2, \"0\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 24-hour day broken into 6 buckets of 4 hours\n",
    "    df = df.withColumn(\"time_bucket\", (F.col(\"hour_24\") / 4).cast(\"int\"))\n",
    "\n",
    "    # time_of_day classification\n",
    "    df = df.withColumn(\n",
    "        \"time_of_day\",\n",
    "        F.when((F.col(\"hour_24\") >= 5) & (F.col(\"hour_24\") <= 11), \"Morning\")\n",
    "         .when((F.col(\"hour_24\") >= 12) & (F.col(\"hour_24\") <= 16), \"Afternoon\")\n",
    "         .when((F.col(\"hour_24\") >= 17) & (F.col(\"hour_24\") <= 20), \"Evening\")\n",
    "         .otherwise(\"Night\")\n",
    "    )\n",
    "\n",
    "    return df.select(\n",
    "        \"Time_Key\",\n",
    "        \"hour_24\",\n",
    "        \"minute\",\n",
    "        \"time_bucket\",\n",
    "        \"time_of_day\",\n",
    "        \"Full_time\"\n",
    "    ).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb91f3ee-309d-496d-abbf-a93b10c33310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, LongType, StringType,\n",
    "    IntegerType, DoubleType\n",
    ")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_location\",\n",
    "    table_properties={\"delta.columnMapping.mode\": \"name\"}\n",
    ")\n",
    "def dim_location():\n",
    "\n",
    "    # Read from Silver layer\n",
    "    src = dlt.read(\"crime_silver\")\n",
    "\n",
    "    # Base unique locations\n",
    "    base = (\n",
    "        src.select(\"LOCATION\", \"CROSS_STREET\", \"AREA\", \"AREA_NAME\", \"LAT\", \"LON\")\n",
    "           .dropDuplicates()\n",
    "    )\n",
    "\n",
    "    # Surrogate key\n",
    "    base = base.withColumn(\n",
    "        \"Location_Key\",\n",
    "        F.when(\n",
    "            F.col(\"LAT\").isNull() | F.col(\"LON\").isNull(),\n",
    "            F.lit(0).cast(\"long\")\n",
    "        ).otherwise(\n",
    "            F.xxhash64(\n",
    "                \"LOCATION\", \"CROSS_STREET\", \"AREA\", \"AREA_NAME\", \"LAT\", \"LON\"\n",
    "            ).cast(\"long\")\n",
    "        )\n",
    "    ).withColumn(\"Source_System\", F.lit(\"LA_CRIME\"))\n",
    "\n",
    "    base = base.select(\n",
    "        \"Location_Key\",\n",
    "        \"LOCATION\",\n",
    "        \"CROSS_STREET\",\n",
    "        \"AREA\",\n",
    "        \"AREA_NAME\",\n",
    "        \"LAT\",\n",
    "        \"LON\",\n",
    "        \"Source_System\"\n",
    "    )\n",
    "\n",
    "    # ---------- UNKNOWN LOCATION ROW (EXPLICIT SCHEMA) ----------\n",
    "    generic_schema = StructType([\n",
    "        StructField(\"Location_Key\", LongType(), False),\n",
    "        StructField(\"LOCATION\", StringType(), True),\n",
    "        StructField(\"CROSS_STREET\", StringType(), True),\n",
    "        StructField(\"AREA\", IntegerType(), True),\n",
    "        StructField(\"AREA_NAME\", StringType(), True),\n",
    "        StructField(\"LAT\", DoubleType(), True),\n",
    "        StructField(\"LON\", DoubleType(), True),\n",
    "        StructField(\"Source_System\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    generic = spark.createDataFrame(\n",
    "        [\n",
    "            (\n",
    "                0,\n",
    "                \"City of Los Angeles (Unknown location)\",\n",
    "                None,\n",
    "                None,\n",
    "                \"City of Los Angeles\",\n",
    "                None,\n",
    "                None,\n",
    "                \"LA_CRIME\"\n",
    "            )\n",
    "        ],\n",
    "        schema=generic_schema\n",
    "    )\n",
    "\n",
    "    # combine base + unknown\n",
    "    df = base.unionByName(generic, allowMissingColumns=True).dropDuplicates([\"Location_Key\"])\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_2_GoldDIMS_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
